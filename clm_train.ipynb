{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Causal language modeling\n\nCausal language models are frequently used for text generation. use these models for creative applications like choosing your own text adventure or an intelligent coding assistant like Copilot or CodeParrot.","metadata":{}},{"cell_type":"code","source":"pip install transformers datasets evaluate accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nhuggingface_token = UserSecretsClient().get_secret(\"huggingface_token\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=huggingface_token)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load ELI5 dataset**","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\neli5 = load_dataset(\"vishnun0027/eli5_dataset\")\neli5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.DataFrame(eli5['train'])\ndf.head(5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"category_counts = df['category'].value_counts()\ncategory_counts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\n# Assuming you have already calculated category_counts using value_counts()\n\n# Create a bar chart\nfig = px.bar(x=category_counts.index, y=category_counts.values, labels={'x':'Category', 'y':'Count'})\n\n# Update layout and set size\nfig.update_layout(title='Count of Each Category', \n                  xaxis_title='Category', \n                  yaxis_title='Count',\n                  width=600,  # Adjust width as needed\n                  height=400  # Adjust height as needed\n                 )\n\n# Show the plot\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load Technology dataset**","metadata":{}},{"cell_type":"code","source":"Technology_df = df[df['category'] == 'Technology']\nTechnology_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\nTech_dataset = Dataset.from_pandas(Technology_df)\nTech_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tech_dataset = Tech_dataset.train_test_split(test_size=0.2)\nTech_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tech_dataset['train'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocess**","metadata":{}},{"cell_type":"markdown","source":"*text field is actually nested inside answers need to extract the text subfield from its nested structure with the flatten method*","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tech_dataset = Tech_dataset.flatten()\nTech_dataset[\"train\"][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_Tech_dataset = Tech_dataset.map(\n    preprocess_function,\n    batched=True,\n    num_proc=4,\n    remove_columns=Tech_dataset[\"train\"].column_names,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"block_size = 128\n\n\ndef group_texts(examples):\n    # Concatenate all texts.\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n    # customize this part to your needs.\n    if total_length >= block_size:\n        total_length = (total_length // block_size) * block_size\n    # Split by chunks of block_size.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm_dataset = tokenized_Tech_dataset.map(group_texts, batched=True, num_proc=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"vishnun0027/tech_clm-model_21042024\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=15,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    push_to_hub=True,\n    report_to=[],\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=lm_dataset[\"train\"],\n    eval_dataset=lm_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\ntrainer.push_to_hub()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\neval_results = trainer.evaluate()\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}